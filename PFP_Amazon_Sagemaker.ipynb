{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8ndStOcrgbn"
   },
   "source": [
    "# **Protein Family Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXqO0yziiIy9"
   },
   "source": [
    "This notebook is intended to be run using Amazon Sagemaker. Sections 1) to 3) of this notebook are very similar to the Protein_Family_Prediction.ipynb file, therefore just a few comments have been included in these sections. More extensive comments in section 4), which is about model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnkx4IIEu_o5"
   },
   "source": [
    "# Importing the dataset and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1688492932991,
     "user": {
      "displayName": "Mattia Perrone",
      "userId": "06361415204114519163"
     },
     "user_tz": 300
    },
    "id": "eQO9plAlFwN2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8ZAyBNGSSA5"
   },
   "source": [
    "## 1) Dataset loading and initial preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwTbR-y3jhiA"
   },
   "source": [
    "The two dataset loaded were previously uploaded to an AWS S3 bucket (\"awsml-159159159\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30099,
     "status": "ok",
     "timestamp": 1688492980247,
     "user": {
      "displayName": "Mattia Perrone",
      "userId": "06361415204114519163"
     },
     "user_tz": 300
    },
    "id": "gp-b3uiJ8mVo",
    "outputId": "2e0bb7da-69cc-4a22-e213-461211c562dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1) Dataset loading and initial preprocessing\n",
    "bucket=\"awsml-159159159\"\n",
    "path1=\"pdb_data_no_dups.csv\"\n",
    "path2=\"pdb_data_seq.csv\"\n",
    "\n",
    "#Loading the dataset\n",
    "def load_data(path1, path2):\n",
    "  data1 = pd.read_csv(path1)\n",
    "  data2 = pd.read_csv(path2)\n",
    "  return data1, data2\n",
    "\n",
    "data1, data2 = load_data(f\"s3://{bucket}/{path1}\", f\"s3://{bucket}/{path2}\")\n",
    "\n",
    "#Merging the two datasets and dropping duplicates (according to the feature \"structureId\")\n",
    "def merge_and_drop_duplicates_id(data1, data2):\n",
    "  data=pd.merge(data1, data2, on=[\"structureId\"])\n",
    "  data.drop_duplicates(subset=['structureId'], inplace=True)\n",
    "  return data\n",
    "\n",
    "data_merged = merge_and_drop_duplicates_id(data1, data2)\n",
    "\n",
    "\n",
    "#Selection of the most common 3 classes of the output variable\n",
    "def classes_selection(data):\n",
    "  classes = ['HYDROLASE', 'TRANSFERASE', 'OXIDOREDUCTASE']\n",
    "  data = data[data.classification.isin(classes)]\n",
    "  ax=sns.countplot(x=data[\"classification\"])\n",
    "  data.replace({\"HYDROLASE\":0,\"TRANSFERASE\":1,\"OXIDOREDUCTASE\":2},inplace=True)\n",
    "  return data\n",
    "\n",
    "data_final = classes_selection(data_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dVCexrASZhv"
   },
   "source": [
    "## 2) Main preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1688492981703,
     "user": {
      "displayName": "Mattia Perrone",
      "userId": "06361415204114519163"
     },
     "user_tz": 300
    },
    "id": "rjuq8cqdhGtK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2) Main preprocessing\n",
    "\n",
    "#Dropping duplicates according to sequence\n",
    "def dropping_duplicates_sequences(data):\n",
    "  data.drop_duplicates(subset='sequence', inplace=True)\n",
    "  data=data[[\"classification\",\"sequence\"]]\n",
    "  data.dropna(subset=[\"sequence\"], inplace=True)\n",
    "  return data\n",
    "\n",
    "data_final=dropping_duplicates_sequences(data_final)\n",
    "\n",
    "\n",
    "\n",
    "#Plotting lengths sequences of proteins\n",
    "def plot_sequence_count(data_final):\n",
    "    sequences=data_final.sequence.values\n",
    "\n",
    "    val_lengths=[]\n",
    "    for i in range(len(sequences)):\n",
    "      val_lengths.append(len(sequences[i]))\n",
    "\n",
    "    sns.histplot(val_lengths, color='g')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Sequence Character Count')\n",
    "    plt.xlim(0,1000)\n",
    "\n",
    "\n",
    "    return sequences, val_lengths\n",
    "\n",
    "sequences, lengths=plot_sequence_count(data_final)\n",
    "\n",
    "\n",
    "\n",
    "#Conversion of amino acids sequences into an integer matrix (x)\n",
    "def conv_to_matrix_and_delete_wrong_measur(max_length, lengths, sequences):\n",
    "  tokenizer = Tokenizer(char_level=True)\n",
    "  tokenizer.fit_on_texts(sequences)\n",
    "  x_seq = tokenizer.texts_to_sequences(sequences)\n",
    "  x = pad_sequences(x_seq, maxlen=max_length)\n",
    "\n",
    "  final=np.arange(x.shape[0])\n",
    "  values_kept=[]\n",
    "  wrong_letters=5\n",
    "  index_row=row=0\n",
    "  for index_row in final:\n",
    "    if (tokenizer.word_index[\"u\"] in x[row,:]) or \\\n",
    "       (tokenizer.word_index[\"x\"] in x[row,:]) or \\\n",
    "       (tokenizer.word_index[\"z\"] in x[row,:]) or \\\n",
    "       (tokenizer.word_index[\"b\"] in x[row,:]) or \\\n",
    "       (tokenizer.word_index[\"o\"] in x[row,:]):\n",
    "       x=np.delete(x, (row), axis=0)\n",
    "       row=row-1\n",
    "    else:\n",
    "      values_kept.append(index_row)\n",
    "    row=row+1\n",
    "\n",
    "  wrong_letters = 5\n",
    "  length_dict = len(tokenizer.word_index) - wrong_letters\n",
    "\n",
    "  return x, values_kept, length_dict, max_length\n",
    "\n",
    "x, values_kept, length_dict, max_length = conv_to_matrix_and_delete_wrong_measur(350,\n",
    "                                                      lengths, sequences)\n",
    "\n",
    "\n",
    "\n",
    "#One hot encoding of the output variable and dataset splitting\n",
    "def ohe_and_dataset_splitting(x, data, values_kept):\n",
    "  lb = LabelBinarizer()\n",
    "  y=data[\"classification\"].iloc[values_kept]\n",
    "  y=lb.fit_transform(y)\n",
    "\n",
    "  x, x_test, y, y_test = train_test_split(x,y, test_size=0.2, random_state=15)\n",
    "  x_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.25, random_state=15)\n",
    "\n",
    "  return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = ohe_and_dataset_splitting(x,\n",
    "                                                        data_final, values_kept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgU3oVSCeX-p"
   },
   "source": [
    "## 3) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1688493002772,
     "user": {
      "displayName": "Mattia Perrone",
      "userId": "06361415204114519163"
     },
     "user_tz": 300
    },
    "id": "JcLbp7uEe3du",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Macro F1-score\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = y_true [:,0]\n",
    "    y_pred = y_pred [:,0]\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_v = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_v\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = y_true [:,0]\n",
    "    y_pred = y_pred [:,0]\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_v = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_v\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    precision_v = precision(y_true, y_pred)\n",
    "    recall_v = recall(y_true, y_pred)\n",
    "    return 2*((precision_v*recall_v)/(precision_v+recall_v+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qQw_Ne7kh-1"
   },
   "source": [
    "## 4) Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model is imported from the an AWS S3 bucket, where it has been previoulsy uploaded, and then loaded using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6257,
     "status": "ok",
     "timestamp": 1688494134108,
     "user": {
      "displayName": "Mattia Perrone",
      "userId": "06361415204114519163"
     },
     "user_tz": 300
    },
    "id": "qiDIus8jt_jf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3=boto3.client('s3')\n",
    "bucket_name='awsml-159159159'\n",
    "file_key='LSTM_model_protein.h5'\n",
    "local_file_path='LSTM_model_protein.h5' \n",
    "\n",
    "s3.download_file(bucket_name, file_key, local_file_path)\n",
    "\n",
    "\n",
    "local_file_path='LSTM_model_protein.h5' \n",
    "LSTM_model_protein=tf.keras.models.load_model(local_file_path, custom_objects={'f1score':f1score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below creates a SageMaker session and get the IAM execution role. Then it specifies the S3 location of the LSTM model, which is stored in h5 format and creates an S3 location for storing the output artifacts, which will be used during the deployment process.\n",
    "\n",
    "Finally it creates a SageMaker Model object and deploy the model using the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session=sagemaker.Session()\n",
    "role=get_execution_role()\n",
    "\n",
    "model_path=f's3://{bucket}/LSTM_model_protein.h5' \n",
    "output_location=f's3://{bucket}/model-artifact/'\n",
    "\n",
    "model=sagemaker.Model(model_data=model_path,\n",
    "                      role=role,\n",
    "                      image_uri=\"763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:2.12.0-cpu-py310-ubuntu20.04-sagemaker\",\n",
    "                      sagemaker_session=sagemaker_session)\n",
    "\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets up the protein_predictor endpoint to handle input data in CSV format using CSVSerializer. It also sets the JSONDeserializer to handle the JSON-formatted predictions returned by the endpoint. Predictions on the test set are stored in the variable results. Finally, the variable predictions is filled by extracting the 'predicted_label' values from the results variable,  which contains the predictions made by the protein_predictor endpoint on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_predictor.serializer=sagemaker.serializers.CSVSerializer()\n",
    "protein_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "results=protein_predictor.predict(x_test)\n",
    "predictions=[]\n",
    "predictions+=[r['predicted_label'] for r in results['predictions']]\n",
    "predictions=np.array(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "51YGtuvjkn8h",
    "GmoFBdl3kwF8"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
